{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Error Handling Comparison: `dspy.Parallel` vs Async\n\nBoth approaches convert errors to strings that the LLM can reason about:\n\n### DSPy.Parallel approach:\n```python\n# Errors automatically become strings\npar = dspy.Parallel(num_threads=5)\noutputs = par.forward(jobs)\n# If a tool fails, output is: \"Error: [details]\"\n```\n\n### Our async approach:\n```python\n# Same behavior with return_exceptions=True\nresults = await asyncio.gather(*tasks, return_exceptions=True)\n# Errors are converted to: \"[ERROR] ExceptionType: details\"\n```\n\n**Key insight**: The LLM receives error information in both cases and can:\n1. See which specific tools failed\n2. Understand why they failed\n3. Decide how to proceed (retry, use alternative tool, etc.)\n\nThis maintains DSPy's philosophy of \"errors as information\" rather than crashes!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate error handling - async maintains DSPy's elegant error handling\nasync def test_error_handling():\n    \"\"\"Show how async handles errors just like dspy.Parallel\"\"\"\n    \n    from agent import async_batch_call\n    \n    # Mix of valid and invalid tool calls\n    calls = [\n        {\"tool_name\": \"web_search\", \"args\": {\"query\": \"valid search\", \"count\": 2}},\n        {\"tool_name\": \"invalid_tool\", \"args\": {\"query\": \"this will fail\"}},  # Invalid tool\n        {\"tool_name\": \"wikipedia_search\", \"args\": {\"query\": \"Python programming\", \"sentences\": 3}},\n        {\"tool_name\": \"web_search\", \"args\": {}},  # Missing required 'query' argument\n    ]\n    \n    print(\"Testing error handling with mixed valid/invalid calls:\\n\")\n    results = await async_batch_call(calls)\n    \n    # Show how errors are converted to strings for LLM reasoning\n    for i, result in enumerate(results):\n        print(f\"Call {i+1}: {result[:150]}...\")\n        if \"[ERROR]\" in result:\n            print(\"  ^ LLM can see this error and reason about what to do next\\n\")\n        else:\n            print(\"  ^ Success\\n\")\n\nawait test_error_handling()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Async Multi-Agent Research System\n\nThis notebook demonstrates the async implementation of our multi-agent research system using DSPy 3.0.\n\n### Key Features:\n1. **True async execution** - All web searches and LLM calls run concurrently\n2. **Clean architecture** - Reusable components in `agent.py`\n3. **Native Jupyter support** - No need for `nest_asyncio` hacks\n4. **Preserved functionality** - All data models and business logic unchanged\n\n### Performance Benefits:\n- Parallel tool execution (3-5x faster)\n- Non-blocking I/O operations\n- Better resource utilization\n- Scalable to many concurrent operations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Advanced usage: Custom async research with direct tool access\nasync def custom_research(topic: str):\n    \"\"\"Example of building custom async research workflows\"\"\"\n    print(f\"Researching: {topic}\\n\")\n    \n    # Phase 1: Parallel reconnaissance\n    recon_calls = [\n        {\"tool_name\": \"web_search\", \"args\": {\"query\": f\"{topic} overview\", \"count\": 2}},\n        {\"tool_name\": \"wikipedia_search\", \"args\": {\"query\": topic, \"sentences\": 5}}\n    ]\n    \n    print(\"Phase 1: Reconnaissance...\")\n    recon_results = await async_batch_call(recon_calls)\n    \n    # Phase 2: Deep dive based on reconnaissance\n    print(\"\\nPhase 2: Deep dive...\")\n    deep_calls = [\n        {\"tool_name\": \"web_search\", \"args\": {\"query\": f\"{topic} latest research 2024\", \"count\": 3}},\n        {\"tool_name\": \"web_search\", \"args\": {\"query\": f\"{topic} expert analysis\", \"count\": 3}}\n    ]\n    \n    deep_results = await async_batch_call(deep_calls)\n    \n    return {\n        \"reconnaissance\": recon_results,\n        \"deep_dive\": deep_results\n    }\n\n# Example usage\nresearch_results = await custom_research(\"multi-agent systems AI\")\nprint(f\"\\nCompleted research with {len(research_results['reconnaissance']) + len(research_results['deep_dive'])} total searches\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport dspy\nimport pydantic\nimport openai\nimport wikipedia\nimport asyncio\nfrom agent import run_research, AsyncLeadAgent, TOOLS\n\n# Print versions\nprint(f\"DSPy version: {dspy.__version__}\")\nprint(f\"Wikipedia version: {wikipedia.__version__}\")\nprint(f\"Pydantic version: {pydantic.__version__}\")\nprint(f\"OpenAI version: {openai.__version__}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Config setup\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Get configuration from environment\nMODEL = os.getenv(\"MODEL_NAME\")\nPLANNER_MODEL = os.getenv(\"PLANNER_MODEL\")\nTEMPERATURE = float(os.getenv(\"TEMPERATURE\", \"1.0\"))\nMAX_TOKENS = int(os.getenv(\"MAX_TOKENS\", \"4000\"))\n\nprint(f\"MODEL: {MODEL}\")\nprint(f\"PLANNER_MODEL: {PLANNER_MODEL}\")\nprint(f\"TEMPERATURE: {TEMPERATURE}\")\nprint(f\"MAX_TOKENS: {MAX_TOKENS}\")\n\n# Configure DSPy with default LM\nlm = dspy.LM(model=MODEL, temperature=TEMPERATURE, max_tokens=MAX_TOKENS)\ndspy.configure(lm=lm)\n\n# Create planner LM for more advanced planning\nplanner_lm = dspy.LM(\n    model=PLANNER_MODEL,\n    temperature=TEMPERATURE,\n    max_tokens=MAX_TOKENS\n)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Test basic async functionality\nasync def test_async():\n    # Test single async web search\n    result = await TOOLS[\"web_search\"].acall(query=\"DSPy framework\", count=2)\n    print(\"Single async search result:\")\n    print(result)\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n    \n    # Test parallel searches\n    from agent import async_batch_call\n    calls = [\n        {\"tool_name\": \"web_search\", \"args\": {\"query\": \"Lamine Yamal stats\", \"count\": 2}},\n        {\"tool_name\": \"web_search\", \"args\": {\"query\": \"Desire Doue stats\", \"count\": 2}},\n        {\"tool_name\": \"wikipedia_search\", \"args\": {\"query\": \"Lamine Yamal\", \"sentences\": 3}}\n    ]\n    \n    results = await async_batch_call(calls)\n    print(\"Parallel search results:\")\n    for i, result in enumerate(results):\n        print(f\"\\nResult {i+1}:\")\n        print(result[:200] + \"...\" if len(result) > 200 else result)\n\n# Run the test\nawait test_async()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Define our research query\nquery = \"Research on Lamine Yamal vs Desire Doue as wingers\"\n\n# Run async research with the planner LM\nanalysis, plan = await run_research(query, planner_lm=planner_lm, verbose=True)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Compare sync vs async performance\nimport time\n\n# Test synchronous approach (simulated)\nprint(\"=== Synchronous Execution (Simulated) ===\")\nstart_sync = time.time()\n\n# Simulate sequential searches (3 seconds each)\nsearch_times = [3, 3, 2]  # Simulated times for web searches\ntotal_sync_time = sum(search_times)\nprint(f\"Sequential execution would take: {total_sync_time} seconds\")\n\n# Test async approach\nprint(\"\\n=== Async Execution (Actual) ===\")\nstart_async = time.time()\n\n# Run multiple searches in parallel\ncalls = [\n    {\"tool_name\": \"web_search\", \"args\": {\"query\": \"Lamine Yamal Barcelona\", \"count\": 3}},\n    {\"tool_name\": \"web_search\", \"args\": {\"query\": \"Desire Doue Rennes\", \"count\": 3}},\n    {\"tool_name\": \"web_search\", \"args\": {\"query\": \"young wingers comparison 2024\", \"count\": 2}}\n]\n\nfrom agent import async_batch_call\nresults = await async_batch_call(calls)\n\nend_async = time.time()\nasync_time = end_async - start_async\n\nprint(f\"Async execution took: {async_time:.2f} seconds\")\nprint(f\"Speed improvement: {total_sync_time / async_time:.1f}x faster!\")\nprint(f\"\\nGot {len(results)} results in parallel\")"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPY SIGNATURE\n",
    "\n",
    "class AnalyzeQuery(dspy.Signature):\n",
    "    \"\"\"Analyze query to determine research strategy. Categorize as depth-first (one topic, multiple angles), breadth-first (multiple independent topics), or straightforward (simple fact).\"\"\"\n",
    "    query: str = dspy.InputField(desc=\"The user's research query\")\n",
    "    analysis: QueryAnalysis = dspy.OutputField(desc=\"Strategic analysis for delegation planning\")\n",
    "\n",
    "class PlanResearch(dspy.Signature):\n",
    "    \"\"\"Create delegation plan for subagents based on analysis. Use tools sparingly for reconnaissance only (verify entities, assess scope). Output specific research tasks for subagents, not research results.\"\"\"\n",
    "    query: str = dspy.InputField(desc=\"The user's research query\")\n",
    "    analysis: QueryAnalysis = dspy.InputField(desc=\"Strategic analysis from previous step\")\n",
    "    plan: ResearchPlan = dspy.OutputField(desc=\"Delegation plan with specific tasks for subagents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Tools\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Enable nested event loops for Jupyter\n",
    "\n",
    "def web_search(query: str, count: int = 5) -> str:\n",
    "    \"\"\"Search the web using Brave Search.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        count: Number of results (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        Formatted search results\n",
    "    \"\"\"\n",
    "    if count > 5:\n",
    "        count = 5\n",
    "    \n",
    "    async def _search():\n",
    "        client = BraveSearch(api_key=BRAVE_SEARCH_API_KEY)\n",
    "        return await client.web(WebSearchRequest(q=query, count=count))\n",
    "    \n",
    "    try:\n",
    "        response = asyncio.run(_search())\n",
    "        \n",
    "        if not response.web or not response.web.results:\n",
    "            return f\"No results found for '{query}'\"\n",
    "\n",
    "        results = []\n",
    "        for i, result in enumerate(response.web.results[:count], 1):\n",
    "            results.append(f\"{i}. {result.title}\\n   {result.description}\\n   {result.url}\")\n",
    "        \n",
    "        return f\"Search results for '{query}':\\n\\n\" + \"\\n\\n\".join(results)\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {e}\"\n",
    "\n",
    "def wikipedia_search(query: str, sentences: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Return a concise English summary for `query` (≤ `sentences` sentences).\n",
    "\n",
    "    If Wikipedia returns multiple possible pages (disambiguation), we list the\n",
    "    top 5 options so the calling agent can decide what to do next.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        wikipedia.set_lang(\"en\")\n",
    "        titles = wikipedia.search(query, results=1)\n",
    "        if not titles:\n",
    "            return f\"No Wikipedia article found for '{query}'.\"\n",
    "\n",
    "        title = titles[0]\n",
    "        summary = wikipedia.summary(title, sentences=sentences, auto_suggest=False)\n",
    "        return f\"Wikipedia – {title}\\n\\n{summary}\"\n",
    "\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        # Show a short disambiguation list\n",
    "        opts = \"\\n • \".join(e.options[:5])\n",
    "        return f\"Wikipedia disambiguation for '{query}'. Try one of:\\n • {opts}\"\n",
    "    except Exception as err:\n",
    "        return f\"Wikipedia error: {err}\"\n",
    "\n",
    "# Register tools\n",
    "TOOLS = {\n",
    "    \"web_search\": dspy.Tool(web_search),\n",
    "    \"wikipedia_search\": dspy.Tool(wikipedia_search),\n",
    "}\n",
    "\n",
    "\n",
    "def batch_tool_call(calls: list[dict]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Execute multiple tool calls in parallel for efficiency.\n",
    "    \n",
    "    Args:\n",
    "        calls: List of dicts, each with:\n",
    "            - tool_name: Name of the tool ('web_search' or 'wikipedia_search')\n",
    "            - args: Dictionary of arguments for that tool\n",
    "            \n",
    "    Example:\n",
    "        calls = [\n",
    "            {\"tool_name\": \"web_search\", \"args\": {\"query\": \"Lamine Yamal stats\", \"count\": 2}},\n",
    "            {\"tool_name\": \"web_search\", \"args\": {\"query\": \"Desire Doue stats\", \"count\": 2}},\n",
    "            {\"tool_name\": \"wikipedia_search\", \"args\": {\"query\": \"Lamine Yamal\", \"sentences\": 5}},\n",
    "            {\"tool_name\": \"wikipedia_search\", \"args\": {\"query\": \"Desire Doue\", \"sentences\": 5}}\n",
    "        ]\n",
    "    \"\"\"\n",
    "    jobs = []\n",
    "    results = []\n",
    "    for call in calls:\n",
    "        tool_name = call.get(\"tool_name\")\n",
    "        args = call.get(\"args\", {})\n",
    "\n",
    "        if not tool_name or tool_name not in TOOLS:\n",
    "            results.append(f\"[ERROR] Unknown tool: {tool_name}\")\n",
    "        else:\n",
    "            jobs.append((TOOLS[tool_name], args))\n",
    "\n",
    "    if jobs:\n",
    "        num_workers = len(calls)\n",
    "        par = dspy.Parallel(num_threads=num_workers)\n",
    "        outputs = par.forward(jobs)\n",
    "        for i, output in enumerate(outputs):\n",
    "            tool = jobs[i][0]\n",
    "            if isinstance(output, Exception):\n",
    "                output = f\"[ERROR] {output}\"\n",
    "            results.append(f\"{tool.name}: {output}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m17:07:27 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash-lite-preview-06-17; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query Analysis ===\n",
      "Type: depth_first\n",
      "Complexity: complex\n",
      "Main concepts: ['Lamine Yamal', 'Desire Doue', 'Winger performance', 'Soccer player comparison']\n",
      "Key entities: ['Lamine Yamal', 'Desire Doue']\n",
      "\n",
      "=== Creating Research Plan ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m17:07:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m17:07:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openrouter/google/gemini-2.5-flash-lite-preview-06-17\n",
      "\u001b[92m17:07:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openrouter/google/gemini-2.5-flash-lite-preview-06-17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2 / 2 examples: 100%|████████████████████████████| 2/2 [00:01<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m17:07:31 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash-lite-preview-06-17; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m17:07:33 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m17:07:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openrouter/google/gemini-2.5-flash-lite-preview-06-17\n",
      "\u001b[92m17:07:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openrouter/google/gemini-2.5-flash-lite-preview-06-17\n",
      "\u001b[92m17:07:33 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash-lite-preview-06-17; provider = openrouter\n",
      "\u001b[92m17:07:35 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m17:07:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openrouter/google/gemini-2.5-flash-lite-preview-06-17\n",
      "\u001b[92m17:07:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openrouter/google/gemini-2.5-flash-lite-preview-06-17\n",
      "\u001b[92m17:07:36 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash-lite-preview-06-17; provider = openrouter\n",
      "\u001b[92m17:07:39 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m17:07:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openrouter/google/gemini-2.5-flash-lite-preview-06-17\n",
      "\u001b[92m17:07:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openrouter/google/gemini-2.5-flash-lite-preview-06-17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2 / 2 examples: 100%|████████████████████████████| 2/2 [00:01<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m17:07:40 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash-lite-preview-06-17; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m17:07:42 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m17:07:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openrouter/google/gemini-2.5-flash-lite-preview-06-17\n",
      "\u001b[92m17:07:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openrouter/google/gemini-2.5-flash-lite-preview-06-17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plan has 2 steps:\n",
      "\n",
      "Step 1: Search for detailed winger performance statistics for Lamine Yamal, focusing on dribbling, crossing, chance creation, goals, assists, and defensive contributions. Use multiple reputable sports statistics websites.\n",
      "  Budget: 5 tool calls\n",
      "  Depends on: []\n",
      "\n",
      "Step 2: Search for detailed winger performance statistics for Desire Doue, focusing on dribbling, crossing, chance creation, goals, assists, and defensive contributions. Use multiple reputable sports statistics websites.\n",
      "  Budget: 5 tool calls\n",
      "  Depends on: [1]\n"
     ]
    }
   ],
   "source": [
    "# Test the improved orchestrator\n",
    "# Phase 1: Analyze the query\n",
    "analysis = query_analysis(query=query)\n",
    "print(\"=== Query Analysis ===\")\n",
    "print(f\"Type: {analysis.analysis.query_type}\")\n",
    "print(f\"Complexity: {analysis.analysis.complexity}\")\n",
    "print(f\"Main concepts: {analysis.analysis.main_concepts}\")\n",
    "print(f\"Key entities: {analysis.analysis.key_entities}\")\n",
    "\n",
    "# Phase 2: Create research plan with minimal reconnaissance\n",
    "print(\"\\n=== Creating Research Plan ===\")\n",
    "plan = research_planner(query=query, analysis=analysis.analysis)\n",
    "print(f\"\\nPlan has {len(plan.plan.steps)} steps:\")\n",
    "for step in plan.plan.steps:\n",
    "    print(f\"\\nStep {step.id}: {step.description}\")\n",
    "    print(f\"  Budget: {step.budget_calls} tool calls\")\n",
    "    print(f\"  Depends on: {step.depends_on}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the improved orchestrator with GPT-4o\n",
    "# Phase 1: Analyze the query\n",
    "analysis = query_analysis(query=query)\n",
    "print(\"=== Query Analysis ===\")\n",
    "print(f\"Type: {analysis.analysis.query_type}\")\n",
    "print(f\"Complexity: {analysis.analysis.complexity}\")\n",
    "print(f\"Main concepts: {analysis.analysis.main_concepts}\")\n",
    "print(f\"Key entities: {analysis.analysis.key_entities}\")\n",
    "\n",
    "# Phase 2: Create research plan with GPT-4o planner\n",
    "print(\"\\n=== Creating Research Plan with GPT-4o ===\")\n",
    "plan = research_planner(query=query, analysis=analysis.analysis)\n",
    "print(f\"\\nPlan has {len(plan.plan.steps)} steps:\")\n",
    "for step in plan.plan.steps:\n",
    "    print(f\"\\nStep {step.id}: {step.description}\")\n",
    "    print(f\"  Budget: {step.budget_calls} tool calls\")\n",
    "    print(f\"  Depends on: {step.depends_on}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}