{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Testing Notebook\n",
    "\n",
    "This notebook tests individual modules from agent.py. \n",
    "\n",
    "**Workflow**: \n",
    "1. Modify agent.py functions\n",
    "2. Run the reload cell below\n",
    "3. Run individual test cells to validate changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent module reloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Reload agent.py after making changes\n",
    "import importlib\n",
    "import agent\n",
    "importlib.reload(agent)\n",
    "from agent import *\n",
    "\n",
    "print(\"Agent module reloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMALL_MODEL: openrouter/google/gemini-2.5-flash-lite-preview-06-17\n",
      "BIG_MODEL: openrouter/openai/gpt-4.1-mini\n",
      "TEMPERATURE: 1.0\n",
      "MAX_TOKENS: 4000\n",
      "\n",
      "Configuration complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup environment and configuration\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import dspy\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Print configuration\n",
    "print(f\"SMALL_MODEL: {SMALL_MODEL}\")\n",
    "print(f\"BIG_MODEL: {BIG_MODEL}\")\n",
    "print(f\"TEMPERATURE: {TEMPERATURE}\")\n",
    "print(f\"MAX_TOKENS: {MAX_TOKENS}\")\n",
    "\n",
    "# Configure DSPy default\n",
    "default_lm = dspy.LM(\n",
    "    model=SMALL_MODEL,\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    api_base=OPENROUTER_BASE_URL,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS\n",
    ")\n",
    "dspy.configure(lm=default_lm)\n",
    "\n",
    "print(\"\\nConfiguration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Individual Async Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test web_search\n",
    "result = await web_search(\"DSPy framework\", count=2)\n",
    "print(\"=== Web Search Test ===\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test wikipedia_search\n",
    "result = wikipedia_search(\"Python programming\", sentences=3)\n",
    "print(\"=== Wikipedia Search Test ===\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test async_batch_call\n",
    "calls = [\n",
    "    {\"tool_name\": \"web_search\", \"args\": {\"query\": \"machine learning\", \"count\": 2}},\n",
    "    {\"tool_name\": \"wikipedia_search\", \"args\": {\"query\": \"artificial intelligence\", \"sentences\": 2}}\n",
    "]\n",
    "\n",
    "results = await async_batch_call(calls)\n",
    "print(\"=== Async Batch Call Test ===\")\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(result[:200] + \"...\" if len(result) > 200 else result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test QueryAnalysis model\n",
    "test_analysis = QueryAnalysis(\n",
    "    query_type=\"depth_first\",\n",
    "    complexity=\"medium\",\n",
    "    main_concepts=[\"AI\", \"machine learning\"],\n",
    "    key_entities=[\"GPT\", \"neural networks\"],\n",
    "    relationships=[\"AI includes machine learning\"],\n",
    "    notes=\"Current state of AI in 2024\",\n",
    "    answer_format=\"detailed analysis\"\n",
    ")\n",
    "\n",
    "print(\"=== QueryAnalysis Model Test ===\")\n",
    "print(f\"Query Type: {test_analysis.query_type}\")\n",
    "print(f\"Complexity: {test_analysis.complexity}\")\n",
    "print(f\"Main Concepts: {test_analysis.main_concepts}\")\n",
    "print(f\"Key Entities: {test_analysis.key_entities}\")\n",
    "print(f\"Answer Format: {test_analysis.answer_format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PlanStep and ResearchPlan models\n",
    "test_step = PlanStep(\n",
    "    id=1,\n",
    "    description=\"Research AI fundamentals\",\n",
    "    depends_on=[],\n",
    "    budget_calls=5\n",
    ")\n",
    "\n",
    "test_plan = ResearchPlan(steps=[test_step])\n",
    "\n",
    "print(\"=== PlanStep and ResearchPlan Test ===\")\n",
    "print(f\"Step ID: {test_step.id}\")\n",
    "print(f\"Description: {test_step.description}\")\n",
    "print(f\"Budget: {test_step.budget_calls}\")\n",
    "print(f\"Plan has {len(test_plan.steps)} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DSPy Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AsyncLeadAgent - Query Analysis\n",
    "test_query = \"Compare machine learning frameworks for beginners\"\n",
    "\n",
    "lead_agent = AsyncLeadAgent()\n",
    "analysis_result = await lead_agent.query_analyzer.acall(query=test_query)\n",
    "\n",
    "print(\"=== AsyncLeadAgent Query Analysis Test ===\")\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"\\nAnalysis:\")\n",
    "print(f\"Type: {analysis_result.analysis.query_type}\")\n",
    "print(f\"Complexity: {analysis_result.analysis.complexity}\")\n",
    "print(f\"Main Concepts: {analysis_result.analysis.main_concepts}\")\n",
    "print(f\"Key Entities: {analysis_result.analysis.key_entities}\")\n",
    "print(f\"Answer Format: {analysis_result.analysis.answer_format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AsyncLeadAgent - Research Planning\n",
    "# Use the analysis from the previous cell\n",
    "plan_result = await lead_agent.planner.acall(\n",
    "    query=test_query,\n",
    "    analysis=analysis_result.analysis\n",
    ")\n",
    "\n",
    "print(\"=== AsyncLeadAgent Research Planning Test ===\")\n",
    "print(f\"Plan has {len(plan_result.plan.steps)} steps:\")\n",
    "\n",
    "for step in plan_result.plan.steps:\n",
    "    print(f\"\\nStep {step.id}: {step.description}\")\n",
    "    print(f\"  Budget: {step.budget_calls} tool calls\")\n",
    "    print(f\"  Depends on: {step.depends_on}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Full AsyncLeadAgent workflow\n",
    "test_query_2 = \"Research the latest developments in quantum computing\"\n",
    "\n",
    "analysis_full, plan_full = await lead_agent.aforward(test_query_2)\n",
    "\n",
    "print(\"=== Full AsyncLeadAgent Workflow Test ===\")\n",
    "print(f\"Query: {test_query_2}\")\n",
    "print(f\"\\n=== Analysis ===\")\n",
    "print(f\"Type: {analysis_full.analysis.query_type}\")\n",
    "print(f\"Complexity: {analysis_full.analysis.complexity}\")\n",
    "print(f\"Main Concepts: {analysis_full.analysis.main_concepts}\")\n",
    "\n",
    "print(f\"\\n=== Plan ===\")\n",
    "print(f\"Generated {len(plan_full.plan.steps)} steps:\")\n",
    "for step in plan_full.plan.steps:\n",
    "    print(f\"\\nStep {step.id}: {step.description}\")\n",
    "    print(f\"  Budget: {step.budget_calls} calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Main Orchestration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run_research function\n",
    "research_query = \"Analyze the impact of AI on software development\"\n",
    "\n",
    "analysis_main, plan_main = await run_research(research_query, verbose=True)\n",
    "\n",
    "print(\"\\n=== run_research Function Test Complete ===\")\n",
    "print(f\"Returned analysis type: {type(analysis_main)}\")\n",
    "print(f\"Returned plan type: {type(plan_main)}\")\n",
    "print(f\"Plan contains {len(plan_main.plan.steps)} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run_research_sync function\n",
    "sync_query = \"Compare Python and JavaScript for web development\"\n",
    "\n",
    "print(\"=== Testing Synchronous Wrapper ===\")\n",
    "sync_analysis, sync_plan = run_research_sync(sync_query, verbose=True)\n",
    "\n",
    "print(f\"\\nSync function completed successfully!\")\n",
    "print(f\"Analysis complexity: {sync_analysis.analysis.complexity}\")\n",
    "print(f\"Plan steps: {len(sync_plan.plan.steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test error handling in async_batch_call\n",
    "error_calls = [\n",
    "    {\"tool_name\": \"web_search\", \"args\": {\"query\": \"valid search\", \"count\": 2}},\n",
    "    {\"tool_name\": \"invalid_tool\", \"args\": {\"query\": \"this will fail\"}},\n",
    "    {\"tool_name\": \"web_search\", \"args\": {}},  # Missing required 'query'\n",
    "]\n",
    "\n",
    "print(\"=== Error Handling Test ===\")\n",
    "error_results = await async_batch_call(error_calls)\n",
    "\n",
    "for i, result in enumerate(error_results):\n",
    "    print(f\"\\nCall {i+1}:\")\n",
    "    if \"[ERROR]\" in result:\n",
    "        print(f\"❌ Error detected: {result[:100]}...\")\n",
    "    else:\n",
    "        print(f\"✅ Success: {result[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance - multiple parallel searches\n",
    "import time\n",
    "\n",
    "performance_calls = [\n",
    "    {\"tool_name\": \"web_search\", \"args\": {\"query\": \"artificial intelligence trends\", \"count\": 2}},\n",
    "    {\"tool_name\": \"web_search\", \"args\": {\"query\": \"machine learning algorithms\", \"count\": 2}},\n",
    "    {\"tool_name\": \"wikipedia_search\", \"args\": {\"query\": \"deep learning\", \"sentences\": 2}},\n",
    "    {\"tool_name\": \"wikipedia_search\", \"args\": {\"query\": \"neural networks\", \"sentences\": 2}}\n",
    "]\n",
    "\n",
    "print(\"=== Performance Test ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "perf_results = await async_batch_call(performance_calls)\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"\\nCompleted {len(performance_calls)} parallel operations in {duration:.2f} seconds\")\n",
    "print(f\"Average time per operation: {duration/len(performance_calls):.2f} seconds\")\n",
    "print(f\"All operations completed successfully: {all('[ERROR]' not in r for r in perf_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools Configuration Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test TOOLS dictionary registration\n",
    "print(\"=== Tools Configuration Test ===\")\n",
    "print(f\"Registered tools: {list(TOOLS.keys())}\")\n",
    "\n",
    "for tool_name, tool in TOOLS.items():\n",
    "    print(f\"\\n{tool_name}:\")\n",
    "    print(f\"  Type: {type(tool)}\")\n",
    "    print(f\"  Name: {getattr(tool, 'name', 'N/A')}\")\n",
    "    print(f\"  Description: {getattr(tool, 'desc', 'N/A')[:50]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
