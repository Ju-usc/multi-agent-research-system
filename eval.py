"""
BrowseComp Evaluation Module - Simplified

Evaluates the existing multi-agent research system on BrowseComp dataset using DSPy evaluation framework.
"""

import dspy
import asyncio
from typing import Optional, Dict, Any
from dataset import BrowseCompDataset
from agent import LeadAgent


class BrowseCompJudge(dspy.Signature):
    """
    Judge whether the following research report correctly answers the question based on the precise and unambiguous correct answer.
    
    Focus only on whether the report contains the correct answer, not on the quality of reasoning or additional information.
    Answer 'True' if the report contains the correct answer (allowing for small variations in wording or format).
    Answer 'False' if the answer is missing, incorrect, or significantly different from the expected answer.
    """
    
    question: str = dspy.InputField(desc="The original BrowseComp question that was asked")
    report: str = dspy.InputField(desc="The full research report generated by the agent")
    correct_answer: str = dspy.InputField(desc="The precise and unambiguous correct answer from the dataset")
    
    extracted_answer: str = dspy.OutputField(desc="The final answer you extracted from the report (or 'None' if no clear answer)")
    reasoning: str = dspy.OutputField(desc="Explain why the extracted answer is correct/incorrect, focusing only on answer matching")
    is_correct: bool = dspy.OutputField(desc="True if extracted answer matches correct answer, False otherwise")


# Create a DSPy program wrapper for the async LeadAgent
class BrowseCompProgram(dspy.Module):
    """DSPy program wrapper for LeadAgent to make it compatible with dspy.Evaluate."""
    
    def __init__(self, agent: LeadAgent):
        super().__init__()
        self.agent = agent
    
    def forward(self, problem: str) -> dspy.Prediction:
        """Forward method required by DSPy - runs the agent and returns prediction."""
        # Run the async agent synchronously 
        report = asyncio.run(self.agent.run(problem))
        return dspy.Prediction(report=report)


def browsecomp_metric(example: dspy.Example, pred: dspy.Prediction, trace=None) -> float:
    """
    DSPy metric function for BrowseComp evaluation.
    
    Args:
        example: DSPy Example with 'problem' and 'answer' fields
        pred: DSPy Prediction with 'report' field
        trace: Optional trace for bootstrapping (not used here)
        
    Returns:
        1.0 if correct, 0.0 if incorrect
    """
    judge = dspy.ChainOfThought(BrowseCompJudge)
    
    try:
        result = judge(
            question=example.problem,
            report=pred.report,
            correct_answer=example.answer
        )
        return 1.0 if result.is_correct else 0.0
    except Exception as e:
        print(f"⚠️ Evaluation error: {e}")
        return 0.0


def run_browsecomp_evaluation(
    num_examples: int = 20,
    num_threads: int = 4,
    agent: Optional[LeadAgent] = None
) -> Dict[str, Any]:
    """
    Run BrowseComp evaluation using DSPy's evaluation framework.
    
    Args:
        num_examples: Number of examples to evaluate
        num_threads: Number of parallel threads
        agent: LeadAgent instance (creates new one if None)
        
    Returns:
        Evaluation results dictionary
    """
    print(f"🎯 Starting BrowseComp Evaluation ({num_examples} examples)")
    
    # Load dataset
    print("📚 Loading BrowseComp dataset...")
    dataset = BrowseCompDataset(num_examples=num_examples)
    examples = dataset.load()
    print(f"✅ Loaded {len(examples)} examples")
    
    # Initialize agent if not provided
    if agent is None:
        print("🤖 Initializing LeadAgent...")
        agent = LeadAgent()
    
    # Create DSPy program wrapper
    program = BrowseCompProgram(agent)
    
    # Create DSPy evaluator
    print(f"⚙️ Setting up evaluation with {num_threads} threads...")
    evaluator = dspy.Evaluate(
        devset=examples,
        metric=browsecomp_metric,
        num_threads=num_threads,
        display_progress=True,
        display_table=5,
        max_errors=10
    )
    
    # Run evaluation
    print("🚀 Starting evaluation...")
    result = evaluator(program)
    
    print(f"\n📊 EVALUATION COMPLETE")
    print(f"✅ Accuracy: {result.score:.1f}%")
    
    return {
        "accuracy": result.score,
        "num_examples": len(examples),
        "results": result.results
    }


if __name__ == "__main__":
    # Example usage
    print("🔍 BrowseComp Evaluation Example")
    print("=" * 50)
    
    # Run evaluation
    results = run_browsecomp_evaluation(num_examples=5, num_threads=2)
    
    print(f"\nFinal Results:")
    print(f"📈 Accuracy: {results['accuracy']:.1f}%")
    print(f"📊 Examples: {results['num_examples']}")
