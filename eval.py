"""
BrowseComp Evaluation Module - Simplified

Evaluates the existing multi-agent research system on BrowseComp dataset using DSPy evaluation framework.
"""

import time
from typing import Any, Callable, Dict, Optional

import dspy
from dspy.adapters.chat_adapter import ChatAdapter
from dspy.teleprompt import GEPA

from agent import Agent
from config import (
    LM_COST_PER_1K_TOKENS,
    WEBSEARCH_COST_PER_CALL_USD,
    TEMPERATURE,
    lm_kwargs_for,
    resolve_model_config,
)
from dataset import BrowseCompDataset
from logging_config import configure_logging
from utils import create_model_cli_parser, iter_model_presets


class BrowseCompJudge(dspy.Signature):
    """
    Judge whether the following research report correctly answers the question based on the precise and unambiguous correct answer.
    
    Focus only on whether the report contains the correct answer, not on the quality of reasoning or additional information.
    Answer 'True' if the report contains the correct answer (allowing for small variations in wording or format).
    Answer 'False' if the answer is missing, incorrect, or significantly different from the expected answer.
    """
    
    question: str = dspy.InputField(desc="The original BrowseComp question that was asked")
    report: str = dspy.InputField(desc="The full research report generated by the agent")
    correct_answer: str = dspy.InputField(desc="The precise and unambiguous correct answer from the dataset")
    
    extracted_answer: str = dspy.OutputField(desc="The final answer you extracted from the report (or 'None' if no clear answer)")
    reasoning: str = dspy.OutputField(desc="Explain why the extracted answer is correct/incorrect, focusing only on answer matching")
    is_correct: bool = dspy.OutputField(desc="True if extracted answer matches correct answer, False otherwise")


# Create a DSPy program wrapper for the async Agent
class BrowseCompProgram(dspy.Module):
    """DSPy program wrapper for Agent to make it compatible with dspy.Evaluate."""

    def __init__(self, agent_factory: Callable[[], Agent] | None = None):
        super().__init__()
        self._agent_factory = agent_factory or Agent

    def forward(self, problem: str) -> dspy.Prediction:
        agent = self._agent_factory()
        agent.web_search_tool.call_count = 0

        start = time.perf_counter()
        agent_prediction = agent(problem)
        elapsed = time.perf_counter() - start

        agent_prediction.report = agent_prediction.answer
        agent_prediction.elapsed_seconds = elapsed
        agent_prediction.websearch_calls = agent.web_search_tool.call_count
        return agent_prediction


def _ensure_lm_configured(
    preset: Optional[str] = None,
    big_override: Optional[str] = None,
    small_override: Optional[str] = None,

) -> Any:
    """Ensure a global LM is registered with DSPy before evaluation runs.

    Returns the resolved model configuration.
    """

    if dspy.settings.lm is not None:
        # Return a config matching the current settings where possible
        return resolve_model_config(preset, big_override, small_override)

    config = resolve_model_config(preset, big_override, small_override)

    dspy.configure(
        lm=dspy.LM(
            model=config.big,
            temperature=TEMPERATURE,
            max_tokens=config.big_max_tokens,
            **lm_kwargs_for(config.big),
        ),
        adapter=ChatAdapter(),
    )

    return config


def browsecomp_metric(example: dspy.Example, pred: dspy.Prediction, trace=None) -> float:
    """
    DSPy metric function for BrowseComp evaluation.
    
    Args:
        example: DSPy Example with 'problem' and 'answer' fields
        pred: DSPy Prediction with 'report' field
        trace: Optional trace for bootstrapping (not used here)
        
    Returns:
        1.0 if correct, 0.0 if incorrect
    """
    judge = dspy.ChainOfThought(BrowseCompJudge)
    
    try:
        result = judge(
            question=example.problem,
            report=pred.report,
            correct_answer=example.answer
        )
        return 1.0 if result.is_correct else 0.0
    except Exception as e:
        print(f"⚠️ Evaluation error: {e}")
        return 0.0


def efficiency_accuracy_metric(example: dspy.Example, pred: dspy.Prediction, trace=None) -> float:
    """Combined metric: accuracy divided by time and cost."""

    judge = dspy.ChainOfThought(BrowseCompJudge)

    try:
        result = judge(
            question=example.problem,
            report=pred.report,
            correct_answer=example.answer,
        )
        accuracy = 1.0 if result.is_correct else 0.0
    except Exception as exc:
        print(f"⚠️ Evaluation error: {exc}")
        return 0.0

    if accuracy == 0.0:
        return 0.0

    usage = pred.get_lm_usage() or {}
    lm_cost = 0.0
    for model, stats in usage.items():
        tokens = stats.get("total_tokens")
        if tokens is None:
            tokens = (stats.get("prompt_tokens", 0) + stats.get("completion_tokens", 0))
        lm_cost += (tokens / 1000.0) * LM_COST_PER_1K_TOKENS.get(model, 0.0)

    web_cost = pred.websearch_calls * WEBSEARCH_COST_PER_CALL_USD
    elapsed = pred.elapsed_seconds

    total_cost = lm_cost + web_cost
    epsilon = 1e-6
    denom_time = max(epsilon, elapsed)
    denom_cost = max(epsilon, total_cost)
    score = accuracy / (denom_time * denom_cost)

    pred.efficiency_breakdown = {
        "accuracy": accuracy,
        "elapsed_seconds": denom_time,
        "lm_cost_usd": lm_cost,
        "web_cost_usd": web_cost,
        "total_cost_usd": total_cost,
        "score": score,
    }

    return score


def run_browsecomp_evaluation(
    num_examples: int = 20,
    num_threads: int = 4,
    agent_factory: Optional[Callable[[], Agent]] = None,
    metric_type: str = "efficiency",
    model_preset: Optional[str] = None,
    model_big: Optional[str] = None,
    model_small: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Run BrowseComp evaluation using DSPy's evaluation framework.
    
    Args:
        num_examples: Number of examples to evaluate
        num_threads: Number of parallel threads
        agent: Agent instance or factory (creates new one if None)
        metric_type: "efficiency" or "accuracy"
        
    Returns:
        Evaluation results dictionary
    """
    print(f"🎯 Starting BrowseComp Evaluation ({num_examples} examples)")
    
    # Load dataset
    print("📚 Loading BrowseComp dataset...")
    dataset = BrowseCompDataset(num_examples=num_examples)
    examples = dataset.load()
    print(f"✅ Loaded {len(examples)} examples")
    
    # Ensure an LM is configured (respect overrides when provided)
    config = _ensure_lm_configured(model_preset, model_big, model_small)

    # Initialize agent if not provided
    if agent_factory is None:
        print("🤖 Initializing Agent factory...")
        agent_factory = lambda: Agent(
            big_model=config.big,
            small_model=config.small,
            temperature=TEMPERATURE,
            big_max_tokens=config.big_max_tokens,
            small_max_tokens=config.small_max_tokens,
        )
    
    # Create DSPy program wrapper
    program = BrowseCompProgram(agent_factory)

    metric_type_normalized = (metric_type or "efficiency").lower()
    metric_fn = {
        "accuracy": browsecomp_metric,
        "efficiency": efficiency_accuracy_metric,
    }.get(metric_type_normalized)

    if metric_fn is None:
        raise ValueError(f"Unknown metric_type '{metric_type}'. Valid options: efficiency, accuracy")

    dspy.settings.configure(track_usage=True)
    
    # Create DSPy evaluator
    print(f"⚙️ Setting up evaluation with {num_threads} threads...")
    evaluator = dspy.Evaluate(
        devset=examples,
        metric=metric_fn,
        num_threads=num_threads,
        display_progress=True,
        display_table=5,
        max_errors=10
    )
    
    # Run evaluation
    print("🚀 Starting evaluation...")
    result = evaluator(program)

    # Normalize to a single return shape: float accuracy and a results list sized to the dataset
    score = float(result)
    details = [None] * len(examples)

    print(f"\n📊 EVALUATION COMPLETE")
    metric_label = "Accuracy" if metric_fn is browsecomp_metric else "Efficiency Score"
    print(f"✅ {metric_label}: {score:.4f}")

    return {
        "metric": score,
        "metric_type": metric_type_normalized,
        "num_examples": len(examples),
        "results": details,
    }


def optimize_browsecomp_with_gepa(
    train_examples: list[dspy.Example],
    agent_factory: Optional[Callable[[], Agent]] = None,
    steps: int = 20,
    metric: Callable[[dspy.Example, dspy.Prediction, Any], float] = efficiency_accuracy_metric,
):
    """Optimize agent prompts on BrowseComp using DSPy's GEPA optimizer."""

    program = BrowseCompProgram(agent_factory)

    dspy.settings.configure(track_usage=True)

    optimizer = GEPA(metric=metric, steps=steps)
    return optimizer.compile(student=program, trainset=train_examples)


def _parse_args():
    parser = create_model_cli_parser(
        "Run BrowseComp evaluation",
        include_list=True,
    )
    parser.add_argument("--num-examples", type=int, default=5, help="Number of dataset examples")
    parser.add_argument("--num-threads", type=int, default=2, help="Parallel evaluation threads")
    parser.add_argument(
        "--metric",
        choices=["efficiency", "accuracy"],
        default="efficiency",
        help="Metric to use during evaluation",
    )
    return parser.parse_args()


def main() -> None:
    args = _parse_args()

    if getattr(args, "list_models", False):
        print("Available presets:")
        for name, preset in iter_model_presets():
            print(f"- {name}: big={preset.big}, small={preset.small}")
        return

    configure_logging()

    print("🔍 BrowseComp Evaluation")
    print("=" * 50)

    results = run_browsecomp_evaluation(
        num_examples=args.num_examples,
        num_threads=args.num_threads,
        metric_type=args.metric,
        model_preset=args.model,
        model_big=args.model_big,
        model_small=args.model_small,
    )

    print("\nFinal Results:")
    print(f"📈 Metric ({results['metric_type']}): {results['metric']:.4f}")
    print(f"📊 Examples: {results['num_examples']}")


if __name__ == "__main__":
    main()
