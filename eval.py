"""
BrowseComp Evaluation Module

Evaluates the multi-agent research system on BrowseComp using DSPy's built-in evaluation framework.
"""

import time
import logging

import dspy
from dspy.adapters.chat_adapter import ChatAdapter
from dspy.teleprompt import GEPA

from agent import Agent
from config import (
    ModelConfig,
    LM_PRICING,
    WEBSEARCH_COST_PER_CALL_USD,
    lm_kwargs_for,
)
from dataset import BrowseCompDataset
from utils import (
    create_model_cli_parser,
    start_cleanup_watchdog,
    create_isolated_workspace,
    cleanup_workspace,
)

logger = logging.getLogger(__name__)

class BrowseCompJudge(dspy.Signature):
    """
    Judge whether the following research report correctly answers the question based on the precise and unambiguous correct answer.
    
    Focus only on whether the report contains the correct answer, not on the quality of reasoning or additional information.
    Answer 'True' if the report contains the correct answer (allowing for small variations in wording or format).
    Answer 'False' if the answer is missing, incorrect, or significantly different from the expected answer.
    """
    
    question: str = dspy.InputField(desc="The original BrowseComp question that was asked")
    report: str = dspy.InputField(desc="The full research report generated by the agent")
    correct_answer: str = dspy.InputField(desc="The precise and unambiguous correct answer from the dataset")
    
    extracted_answer: str = dspy.OutputField(desc="The final answer you extracted from the report (or 'None' if no clear answer)")
    reasoning: str = dspy.OutputField(desc="Explain why the extracted answer is correct/incorrect, focusing only on answer matching")
    is_correct: bool = dspy.OutputField(desc="True if extracted answer matches correct answer, False otherwise")


class BrowseCompProgram(dspy.Module):
    """
    DSPy program wrapper for Agent to make it compatible with dspy.Evaluate.

    Agent is created as module attribute so GEPA can discover and optimize tools.
    Uses deepcopy() for thread-safe parallel evaluation.
    """

    def __init__(self, config: ModelConfig | None = None):
        super().__init__()
        self.agent = Agent(config=config, work_dir="memory_eval/default")

    def forward(self, problem: str) -> dspy.Prediction:
        work_dir = create_isolated_workspace()

        try:
            agent = self.agent.deepcopy()
            agent.reset_workspace(work_dir)

            start = time.perf_counter()
            prediction = agent(problem)
            elapsed = time.perf_counter() - start

            prediction.report = prediction.answer
            prediction.elapsed_seconds = elapsed
            prediction.websearch_calls = agent.web_search_tool.call_count

            return prediction
        finally:
            cleanup_workspace(work_dir)

def calculate_lm_cost(usage: dict) -> float:
    """Calculate LM cost with accurate input/output/cached token pricing.

    Pricing in LM_PRICING is per 1M tokens (industry standard).
    Formula: (tokens / 1,000,000) * price_per_1M = cost in USD
    """
    total_cost = 0.0

    for model_name, stats in usage.items():
        pricing = LM_PRICING.get(model_name, {})
        if not pricing:
            logger.warning(f"No pricing configured for model: {model_name}")
            continue

        prompt_tokens = stats.get("prompt_tokens", 0)
        completion_tokens = stats.get("completion_tokens", 0)
        prompt_details = stats.get("prompt_tokens_details") or {}
        cached_tokens = prompt_details.get("cached_tokens", 0)
        non_cached_input = prompt_tokens - cached_tokens

        # Pricing is per 1M tokens, so divide by 1,000,000
        input_cost = (non_cached_input / 1_000_000) * pricing.get("input", 0.0)
        cached_cost = (cached_tokens / 1_000_000) * pricing.get("cached_input", pricing.get("input", 0.0))
        output_cost = (completion_tokens / 1_000_000) * pricing.get("output", 0.0)

        total_cost += input_cost + cached_cost + output_cost

    return total_cost


class BrowseCompEvaluator:
    """Encapsulates BrowseComp evaluation with proper state management."""

    def __init__(
        self,
        config: ModelConfig,
        num_threads: int = 2,
        optimize_steps: int = 10,
    ):
        self.config = config
        self.num_threads = num_threads
        self.optimize_steps = optimize_steps

        # Initialize grader LM once for all evaluations (major efficiency improvement)
        self.grader_lm = dspy.LM(
            model=config.grader,
            temperature=1.0,  # Required for GPT-5 reasoning models
            max_tokens=config.grader_max_tokens,
            **lm_kwargs_for(config.grader),
        )
        self.judge = dspy.ChainOfThought(BrowseCompJudge)

        # Lazy init: reflection LM only created when optimize_with_gepa is called
        self._reflection_lm = None
    
    def judge_prediction(self, example: dspy.Example, pred: dspy.Prediction) -> float:
        """Judge single prediction using initialized grader LM."""
        try:
            with dspy.context(lm=self.grader_lm):
                result = self.judge(
                    question=example.problem,
                    report=pred.report,
                    correct_answer=example.answer
                )
            return 1.0 if result.is_correct else 0.0
        except Exception as e:
            logger.error(f"Evaluation error: {e}")
            return 0.0
    
    def calculate_metrics(self, example: dspy.Example, pred: dspy.Prediction) -> dict:
        """Calculate all metrics (accuracy, cost, time) for a prediction."""
        accuracy = self.judge_prediction(example, pred)
        usage = pred.get_lm_usage() or {}
        lm_cost_usd = calculate_lm_cost(usage)
        web_cost_usd = pred.websearch_calls * WEBSEARCH_COST_PER_CALL_USD
        elapsed = pred.elapsed_seconds
        total_cost_usd = lm_cost_usd + web_cost_usd

        return {
            "accuracy": accuracy,
            "elapsed_seconds": elapsed,
            "total_cost_usd": total_cost_usd,
            "lm_cost_usd": lm_cost_usd,
            "web_cost_usd": web_cost_usd,
            "websearch_calls": pred.websearch_calls,
            "lm_usage": usage,
        }
    
    def metric(self, example: dspy.Example, pred: dspy.Prediction, trace=None) -> float:
        """DSPy metric for evaluation. Returns accuracy and stores full metrics."""
        metrics = self.calculate_metrics(example, pred)
        pred.metrics = metrics
        return metrics["accuracy"]

    def optimize_with_gepa(self, program: BrowseCompProgram, train: list) -> BrowseCompProgram:
        """Run GEPA optimization on program."""
        # Lazy init reflection LM on first optimization call
        if self._reflection_lm is None:
            self._reflection_lm = dspy.LM(
                model=self.config.reflector,
                temperature=1.0,
                max_tokens=self.config.reflector_max_tokens,
                **lm_kwargs_for(self.config.reflector),
            )

        optimizer = GEPA(
            metric=self.metric,
            reflection_lm=self._reflection_lm,
            max_full_evals=self.optimize_steps,
            num_threads=self.num_threads,
            track_stats=True,
            track_best_outputs=True,
            candidate_selection_strategy='pareto',
            use_merge=True,
            optimize_tool_descriptions=True,
        )

        return optimizer.compile(student=program, trainset=train)

    def run(self, program: BrowseCompProgram, examples: list) -> tuple:
        """Run evaluation and return (result, predictions)."""
        predictions_dict = {}

        def metric_with_capture(example, pred, trace=None):
            score = self.metric(example, pred, trace)
            predictions_dict[example.problem] = pred
            return score

        evaluator = dspy.Evaluate(
            devset=examples,
            metric=metric_with_capture,
            num_threads=self.num_threads,
            display_progress=True,
            display_table=5,
            max_errors=10,
        )
        
        result = evaluator(program)
        predictions = self._extract_predictions(predictions_dict, examples)
        return result, predictions
    
    def _extract_predictions(self, predictions_dict: dict, examples: list) -> list:
        """Extract predictions in correct order, handling missing ones."""
        predictions = []
        for i, ex in enumerate(examples):
            pred = predictions_dict.get(ex.problem)
            if pred is None:
                logger.warning(f"Missing prediction for example {i}, creating placeholder")
                pred = dspy.Prediction(answer="ERROR", report="ERROR")
                pred.metrics = {"accuracy": 0.0, "elapsed_seconds": 0, "total_cost_usd": 0}
            predictions.append(pred)
        return predictions

def _parse_args():
    parser = create_model_cli_parser("Run BrowseComp evaluation")
    parser.add_argument("--num-examples", type=int, default=10, help="Number of dataset examples")
    parser.add_argument("--num-threads", type=int, default=2, help="Parallel evaluation threads")
    parser.add_argument("--optimize", action="store_true", help="Run GEPA optimization")
    parser.add_argument("--optimize-steps", type=int, default=10)
    parser.add_argument("--train-size", type=float, default=0.7)
    return parser.parse_args()


def main() -> None:
    args = _parse_args()
    logging.basicConfig(level=logging.INFO)

    print("ğŸ” BrowseComp Evaluation")
    print("=" * 50)

    # Build config from CLI args
    config = ModelConfig(lead=args.lead, sub=args.sub)
    print(f"âš–ï¸  Grader model: {config.grader}")
    print("=" * 50)
    print(f"ğŸ¤– Models: lead={config.lead}, sub={config.sub}")

    if dspy.settings.lm is None:
        dspy.configure(
            lm=dspy.LM(
                model=config.lead,
                temperature=config.temperature,
                max_tokens=config.lead_max_tokens,
                **lm_kwargs_for(config.lead),
            ),
            adapter=ChatAdapter(),
        )

    dspy.settings.configure(track_usage=True)

    # Initialize evaluator
    evaluator = BrowseCompEvaluator(
        config=config,
        num_threads=args.num_threads,
        optimize_steps=args.optimize_steps,
    )

    # Load dataset
    dataset = BrowseCompDataset(num_examples=args.num_examples)
    examples = dataset.load()
    print(f"ğŸ“š Loaded {len(examples)} examples")

    # Create agent program
    program = BrowseCompProgram(config=config)

    # GEPA optimization if requested
    if args.optimize:
        print(f"\nğŸ§¬ GEPA Optimization ({args.optimize_steps} steps)")
        print(f"ğŸ¤– Using reflector model: {config.reflector}")
        train, test = dataset.split(train_size=args.train_size)
        print(f"ğŸ“Š Split: {len(train)} train, {len(test)} test")
        
        program = evaluator.optimize_with_gepa(program, train)
        
        print("\nâœ¨ Optimization complete!")
        print(f"ğŸ“ Optimized {len(list(program.named_predictors()))} predictor(s)")
        for name, pred in program.named_predictors():
            instr = getattr(pred.signature, 'instructions', '<no instructions>')
            print(f"  â€¢ {name}: {instr[:80]}..." if len(instr) > 80 else f"  â€¢ {name}: {instr}")
        
        examples = test  # Evaluate on test set

    # Run evaluation
    print("ğŸš€ Evaluating...")
    result, predictions = evaluator.run(program, examples)

    # Workaround for DSPy/LiteLLM cleanup hang
    start_cleanup_watchdog(grace_period_seconds=30)

    print("\n" + "=" * 50)
    print(f"ğŸ“ˆ Accuracy Score: {result.score:.4f}")
    print(f"ğŸ“Š Examples: {len(examples)}")
    if args.optimize:
        print(f"ğŸ§¬ Optimized with GEPA ({args.optimize_steps} steps)")


if __name__ == "__main__":
    main()
