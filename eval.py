"""
BrowseComp Evaluation Module

Evaluates the multi-agent research system on BrowseComp using DSPy's built-in evaluation framework.
"""

import time
import logging
from pathlib import Path

import dspy
from dspy.adapters.chat_adapter import ChatAdapter
from dspy.teleprompt import GEPA

from agent import Agent
from config import (
    WEBSEARCH_COST_PER_CALL_USD,
    TEMPERATURE,
    GRADER_MODEL,
    GRADER_MAX_TOKENS,
    OPTIMIZER_MODEL,
    OPTIMIZER_MAX_TOKENS,
    lm_kwargs_for,
    resolve_model_config,
)
from dataset import BrowseCompDataset
from utils import (
    create_model_cli_parser,
    iter_model_presets,
    start_cleanup_watchdog,
    create_isolated_workspace,
    cleanup_workspace,
    calculate_lm_cost,
)

logger = logging.getLogger(__name__)

class BrowseCompJudge(dspy.Signature):
    """
    Judge whether the following research report correctly answers the question based on the precise and unambiguous correct answer.
    
    Focus only on whether the report contains the correct answer, not on the quality of reasoning or additional information.
    Answer 'True' if the report contains the correct answer (allowing for small variations in wording or format).
    Answer 'False' if the answer is missing, incorrect, or significantly different from the expected answer.
    """
    
    question: str = dspy.InputField(desc="The original BrowseComp question that was asked")
    report: str = dspy.InputField(desc="The full research report generated by the agent")
    correct_answer: str = dspy.InputField(desc="The precise and unambiguous correct answer from the dataset")
    
    extracted_answer: str = dspy.OutputField(desc="The final answer you extracted from the report (or 'None' if no clear answer)")
    reasoning: str = dspy.OutputField(desc="Explain why the extracted answer is correct/incorrect, focusing only on answer matching")
    is_correct: bool = dspy.OutputField(desc="True if extracted answer matches correct answer, False otherwise")


class BrowseCompProgram(dspy.Module):
    """
    DSPy program wrapper for Agent to make it compatible with dspy.Evaluate.
    
    Agent is created as module attribute so GEPA can discover and optimize tools.
    Uses deepcopy() for thread-safe parallel evaluation.
    """

    def __init__(self, big_model: str, small_model: str, big_max_tokens: int, small_max_tokens: int):
        super().__init__()
        self.agent = Agent(
            big_model=big_model,
            small_model=small_model,
            temperature=TEMPERATURE,
            big_max_tokens=big_max_tokens,
            small_max_tokens=small_max_tokens,
            work_dir="memory_eval/default",
        )

    def forward(self, problem: str) -> dspy.Prediction:
        work_dir = create_isolated_workspace()
        
        try:
            # Use DSPy's built-in deepcopy to preserve optimized tool descriptions
            agent = self.agent.deepcopy()
            agent.fs_tool.root = Path(work_dir)  # Update filesystem root
            
            # Get the actual web_search_tool used by subagents (may differ after deepcopy)
            # DSPy's shallow copy of subagent_tools means the func reference is preserved
            actual_ws_tool = agent.subagent_tools["web_search"].func
            actual_ws_tool.call_count = 0

            start = time.perf_counter()
            prediction = agent(problem)
            elapsed = time.perf_counter() - start
            
            prediction.report = prediction.answer
            prediction.elapsed_seconds = elapsed
            prediction.websearch_calls = actual_ws_tool.call_count
            
            return prediction
        finally:
            cleanup_workspace(work_dir)

class BrowseCompEvaluator:
    """Encapsulates BrowseComp evaluation with proper state management."""
    
    def __init__(self, args):
        self.args = args
        
        # Initialize grader LM once for all evaluations (major efficiency improvement)
        self.grader_lm = dspy.LM(
            model=GRADER_MODEL,
            temperature=1.0,  # Required for GPT-5 reasoning models
            max_tokens=GRADER_MAX_TOKENS,
            **lm_kwargs_for(GRADER_MODEL),
        )
        self.judge = dspy.ChainOfThought(BrowseCompJudge)
        
        # Initialize reflection LM for GEPA optimization if needed
        if args.optimize:
            self.reflection_lm = dspy.LM(
                model=OPTIMIZER_MODEL,
                temperature=1.0,  # Higher temp for creative prompt mutations
                max_tokens=OPTIMIZER_MAX_TOKENS,
                **lm_kwargs_for(OPTIMIZER_MODEL),
            )
    
    def judge_prediction(self, example: dspy.Example, pred: dspy.Prediction) -> tuple[float, dict]:
        """Judge single prediction using initialized grader LM.
        
        Returns:
            tuple: (score, judge_result_dict) where judge_result_dict contains
                   extracted_answer, reasoning, is_correct for diagnostic use
        """
        try:
            with dspy.context(lm=self.grader_lm):
                result = self.judge(
                    question=example.problem,
                    report=pred.report,
                    correct_answer=example.answer
                )
            score = 1.0 if result.is_correct else 0.0
            result_dict = {
                "extracted_answer": result.extracted_answer,
                "reasoning": result.reasoning,
                "is_correct": result.is_correct,
            }
            return score, result_dict
        except Exception as e:
            logger.error(f"Evaluation error: {e}")
            return 0.0, {"extracted_answer": "ERROR", "reasoning": str(e), "is_correct": False}
    
    def calculate_metrics(self, example: dspy.Example, pred: dspy.Prediction) -> dict:
        """Calculate all metrics (accuracy, cost, time) for a prediction."""
        accuracy, _ = self.judge_prediction(example, pred)
        usage = pred.get_lm_usage() or {}
        lm_cost = calculate_lm_cost(usage)
        web_cost = pred.websearch_calls * WEBSEARCH_COST_PER_CALL_USD
        elapsed = pred.elapsed_seconds
        total_cost = lm_cost + web_cost
        
        return {
            "accuracy": accuracy,
            "elapsed_seconds": elapsed,
            "total_cost_usd": total_cost,
            "lm_cost_usd": lm_cost,
            "web_cost_usd": web_cost,
            "websearch_calls": pred.websearch_calls,
            "lm_usage": usage,
            "efficiency_temp": accuracy / (max(1e-6, elapsed) * max(1e-6, total_cost)) if accuracy > 0 else 0.0,
        }
    
    def accuracy_metric(self, example: dspy.Example, pred: dspy.Prediction, trace=None, pred_name=None, pred_trace=None):
        """DSPy metric for accuracy. Returns float or Prediction with feedback for GEPA.
        
        - pred_name=None: return float score (module-level)
        - pred_name set: return Prediction with feedback (predictor-level for GEPA)
        """
        score, judge_result = self.judge_prediction(example, pred)
        
        if pred_name is None:
            return score
        
        # GEPA wants feedback for predictor-level optimization
        feedback = f"Expected: {example.answer}\nGot: {judge_result['extracted_answer']}\nReasoning: {judge_result['reasoning']}"
        return dspy.Prediction(score=score, feedback=feedback)

    def efficiency_metric(self, example: dspy.Example, pred: dspy.Prediction, trace=None, pred_name=None, pred_trace=None):
        """DSPy metric: returns accuracy, stores full metrics in prediction.
        
        GEPA requires 5 arguments: (gold, pred, trace, pred_name, pred_trace).
        """
        metrics = self.calculate_metrics(example, pred)
        pred.metrics = metrics
        return metrics["accuracy"]
    
    def optimize_with_gepa(self, program: BrowseCompProgram, train: list) -> BrowseCompProgram:
        """Run GEPA optimization on program."""
        metric_fn = self.accuracy_metric
        
        optimizer = GEPA(
            metric=metric_fn,
            reflection_lm=self.reflection_lm,
            max_full_evals=self.args.optimize_steps,  # Use explicit steps (can't combine with auto)
            num_threads=self.args.num_threads,
            track_stats=True,
            track_best_outputs=True,
            candidate_selection_strategy='pareto',
            use_merge=True,
            optimize_tool_descriptions=True,  # Optimize tool descriptions alongside signatures
        )
        
        return optimizer.compile(student=program, trainset=train)
    
    def run(self, program: BrowseCompProgram, examples: list) -> tuple:
        """Run evaluation and return (result, predictions)."""
        metric_fn = self.efficiency_metric if self.args.metric == "efficiency" else self.accuracy_metric
        
        evaluator = dspy.Evaluate(
            devset=examples,
            metric=metric_fn,
            num_threads=self.args.num_threads,
            display_progress=True,
            display_table=5,
            max_errors=10,
        )
        
        result = evaluator(program)
        
        # Extract predictions from DSPy's built-in result.results
        # Each item is (example, prediction, score)
        predictions = [pred for _, pred, _ in result.results]
        return result, predictions

def _parse_args():
    parser = create_model_cli_parser("Run BrowseComp evaluation", include_list=True)
    parser.add_argument("--num-examples", type=int, default=10, help="Number of dataset examples")
    parser.add_argument("--num-threads", type=int, default=2, help="Parallel evaluation threads")
    parser.add_argument("--metric", choices=["efficiency", "accuracy"], default="efficiency")
    parser.add_argument("--optimize", action="store_true", help="Run GEPA optimization")
    parser.add_argument("--optimize-steps", type=int, default=10)
    parser.add_argument("--train-size", type=float, default=0.7)
    parser.add_argument("--save-metrics", type=str, help="Save detailed metrics to JSON")
    return parser.parse_args()


def main() -> None:
    args = _parse_args()

    if getattr(args, "list_models", False):
        print("Available presets:")
        for name, preset in iter_model_presets():
            print(f"- {name}: big={preset.big}, small={preset.small}")
        return

    logging.basicConfig(level=logging.INFO)

    print("üîç BrowseComp Evaluation")
    print("=" * 50)
    print(f"‚öñÔ∏è  Grader model: {GRADER_MODEL} (fixed for consistency)")
    print("=" * 50)

    # Configure DSPy and prepare agent
    config = resolve_model_config(args.model, args.model_big, args.model_small)
    
    if dspy.settings.lm is None:
        dspy.configure(
            lm=dspy.LM(
                model=config.big,
                temperature=TEMPERATURE,
                max_tokens=config.big_max_tokens,
                **lm_kwargs_for(config.big),
            ),
            adapter=ChatAdapter(),
        )
    
    dspy.settings.configure(track_usage=True)
    
    # Initialize evaluator with grader and optimizer LMs
    evaluator = BrowseCompEvaluator(args)
    
    # Load dataset
    dataset = BrowseCompDataset(num_examples=args.num_examples)
    examples = dataset.load()
    print(f"üìö Loaded {len(examples)} examples")

    # Create agent program
    program = BrowseCompProgram(
        big_model=config.big,
        small_model=config.small,
        big_max_tokens=config.big_max_tokens,
        small_max_tokens=config.small_max_tokens,
    )

    # GEPA optimization if requested
    if args.optimize:
        print(f"\nüß¨ GEPA Optimization ({args.optimize_steps} steps)")
        print(f"ü§ñ Using reflection model: {OPTIMIZER_MODEL}")
        train, test = dataset.split(train_size=args.train_size)
        print(f"üìä Split: {len(train)} train, {len(test)} test")
        
        program = evaluator.optimize_with_gepa(program, train)
        
        print("\n‚ú® Optimization complete!")
        print(f"üìù Optimized {len(list(program.named_predictors()))} predictor(s)")
        for name, pred in program.named_predictors():
            instr = getattr(pred.signature, 'instructions', '<no instructions>')
            print(f"  ‚Ä¢ {name}: {instr[:80]}..." if len(instr) > 80 else f"  ‚Ä¢ {name}: {instr}")
        
        examples = test  # Evaluate on test set

    # Run evaluation
    print("üöÄ Evaluating...")
    result, predictions = evaluator.run(program, examples)

    # Workaround for DSPy/LiteLLM cleanup hang
    start_cleanup_watchdog(grace_period_seconds=30)

    print("\n" + "=" * 50)
    print(f"üìà {args.metric.title()} Score: {result.score:.4f}")
    print(f"üìä Examples: {len(examples)}")
    if args.optimize:
        print(f"üß¨ Optimized with GEPA ({args.optimize_steps} steps)")


if __name__ == "__main__":
    main()
