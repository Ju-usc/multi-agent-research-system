"""
BrowseComp Evaluation Module

Evaluates the multi-agent research system on BrowseComp using DSPy's built-in evaluation framework.
"""

import time
from typing import Any, Callable, Optional

import dspy
from dspy.adapters.chat_adapter import ChatAdapter
from dspy.teleprompt import GEPA

from agent import Agent
from config import (
    LM_COST_PER_1K_TOKENS,
    WEBSEARCH_COST_PER_CALL_USD,
    TEMPERATURE,
    lm_kwargs_for,
    resolve_model_config,
)
from dataset import BrowseCompDataset
from logging_config import configure_logging
from utils import create_model_cli_parser, iter_model_presets, save_experiment_results


class BrowseCompJudge(dspy.Signature):
    """
    Judge whether the following research report correctly answers the question based on the precise and unambiguous correct answer.
    
    Focus only on whether the report contains the correct answer, not on the quality of reasoning or additional information.
    Answer 'True' if the report contains the correct answer (allowing for small variations in wording or format).
    Answer 'False' if the answer is missing, incorrect, or significantly different from the expected answer.
    """
    
    question: str = dspy.InputField(desc="The original BrowseComp question that was asked")
    report: str = dspy.InputField(desc="The full research report generated by the agent")
    correct_answer: str = dspy.InputField(desc="The precise and unambiguous correct answer from the dataset")
    
    extracted_answer: str = dspy.OutputField(desc="The final answer you extracted from the report (or 'None' if no clear answer)")
    reasoning: str = dspy.OutputField(desc="Explain why the extracted answer is correct/incorrect, focusing only on answer matching")
    is_correct: bool = dspy.OutputField(desc="True if extracted answer matches correct answer, False otherwise")


# Create a DSPy program wrapper for the async Agent
class BrowseCompProgram(dspy.Module):
    """
    DSPy program wrapper for Agent to make it compatible with dspy.Evaluate.
    
    Creates isolated workspace per example to prevent filesystem conflicts
    when running with --num-threads > 1.
    """

    def __init__(self, agent_factory: Callable[[str | None], Agent] | None = None):
        super().__init__()
        self._agent_factory = agent_factory or Agent

    def forward(self, problem: str) -> dspy.Prediction:
        import uuid
        from pathlib import Path
        import shutil
        
        # Create isolated workspace for this example (prevents parallel conflicts)
        work_dir = Path("memory_eval") / str(uuid.uuid4())[:8]
        work_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # Create agent with isolated workspace
            agent = self._agent_factory(work_dir=str(work_dir))
            agent.web_search_tool.call_count = 0

            start = time.perf_counter()
            agent_prediction = agent(problem)
            elapsed = time.perf_counter() - start

            agent_prediction.report = agent_prediction.answer
            agent_prediction.elapsed_seconds = elapsed
            agent_prediction.websearch_calls = agent.web_search_tool.call_count
            
            return agent_prediction
        finally:
            # Cleanup workspace after this example (best effort)
            try:
                shutil.rmtree(work_dir)
            except Exception:
                pass  # Don't fail evaluation if cleanup fails

def browsecomp_metric(example: dspy.Example, pred: dspy.Prediction, trace=None) -> float:
    """
    DSPy metric function for BrowseComp evaluation.
    
    Args:
        example: DSPy Example with 'problem' and 'answer' fields
        pred: DSPy Prediction with 'report' field
        trace: Optional trace for bootstrapping (not used here)
        
    Returns:
        1.0 if correct, 0.0 if incorrect
    """
    judge = dspy.ChainOfThought(BrowseCompJudge)
    
    try:
        result = judge(
            question=example.problem,
            report=pred.report,
            correct_answer=example.answer
        )
        return 1.0 if result.is_correct else 0.0
    except Exception as e:
        print(f"‚ö†Ô∏è Evaluation error: {e}")
        return 0.0


def efficiency_accuracy_metric(example: dspy.Example, pred: dspy.Prediction, trace=None) -> float:
    """
    Metric for data collection phase.
    Returns accuracy but stores ALL raw metrics for analysis.
    
    NOTE: Current time√ócost formula is incomplete - will be revised
    after analyzing empirical distributions.
    """
    # Reuse accuracy calculation (avoid duplicate judge call)
    accuracy = browsecomp_metric(example, pred, trace)

    # Calculate costs (token fallback required for different providers)
    usage = pred.get_lm_usage() or {}
    lm_cost = 0.0
    for model, stats in usage.items():
        # Different providers return different formats:
        # - OpenAI/LiteLLM: {total_tokens, prompt_tokens, completion_tokens}
        # - Others may only have: {prompt_tokens, completion_tokens}
        # This fallback is empirically necessary and correct.
        tokens = stats.get("total_tokens")
        if tokens is None:
            tokens = (stats.get("prompt_tokens", 0) + stats.get("completion_tokens", 0))
        lm_cost += (tokens / 1000.0) * LM_COST_PER_1K_TOKENS.get(model, 0.0)

    web_cost = pred.websearch_calls * WEBSEARCH_COST_PER_CALL_USD
    elapsed = pred.elapsed_seconds
    total_cost = lm_cost + web_cost

    # Store RAW metrics (for empirical analysis)
    pred.metrics = {
        "accuracy": accuracy,
        
        # Raw dimensions (for analysis - don't combine yet)
        "elapsed_seconds": elapsed,
        "total_cost_usd": total_cost,
        "lm_cost_usd": lm_cost,
        "web_cost_usd": web_cost,
        "websearch_calls": pred.websearch_calls,
        
        # Token details
        "lm_usage": usage,
        
        # Temporary: keep current formula for comparison
        "efficiency_temp": accuracy / (max(1e-6, elapsed) * max(1e-6, total_cost)) if accuracy > 0 else 0.0,
    }

    # Return accuracy (optimize for correctness during data collection)
    return accuracy

def _parse_args():
    parser = create_model_cli_parser(
        "Run BrowseComp evaluation",
        include_list=True,
    )
    parser.add_argument("--num-examples", type=int, default=10, help="Number of dataset examples")
    parser.add_argument("--num-threads", type=int, default=2, help="Parallel evaluation threads")
    parser.add_argument(
        "--metric",
        choices=["efficiency", "accuracy"],
        default="efficiency",
        help="Metric to use during evaluation",
    )
    parser.add_argument(
        "--optimize",
        action="store_true",
        help="Run GEPA optimization before evaluation",
    )
    parser.add_argument(
        "--optimize-steps",
        type=int,
        default=10,
        help="Number of GEPA optimization steps (default: 10)",
    )
    parser.add_argument(
        "--train-size",
        type=float,
        default=0.7,
        help="Train/test split ratio for GEPA optimization (default: 0.7)",
    )
    parser.add_argument(
        "--save-metrics",
        type=str,
        help="Save detailed metrics to JSON file (e.g., results.json)",
    )
    return parser.parse_args()


def main() -> None:
    args = _parse_args()

    if getattr(args, "list_models", False):
        print("Available presets:")
        for name, preset in iter_model_presets():
            print(f"- {name}: big={preset.big}, small={preset.small}")
        return

    configure_logging()

    print("üîç BrowseComp Evaluation")
    print("=" * 50)

    # Configure DSPy and prepare agent
    config = resolve_model_config(args.model, args.model_big, args.model_small)
    
    if dspy.settings.lm is None:
        dspy.configure(
            lm=dspy.LM(
                model=config.big,
                temperature=TEMPERATURE,
                max_tokens=config.big_max_tokens,
                **lm_kwargs_for(config.big),
            ),
            adapter=ChatAdapter(),
        )
    
    dspy.settings.configure(track_usage=True)
    
    # Agent factory with work_dir support for isolated workspaces
    def agent_factory(work_dir: str | None = None):
        return Agent(
            big_model=config.big,
            small_model=config.small,
            temperature=TEMPERATURE,
            big_max_tokens=config.big_max_tokens,
            small_max_tokens=config.small_max_tokens,
            work_dir=work_dir,
        )
    
    metric_fn = efficiency_accuracy_metric if args.metric == "efficiency" else browsecomp_metric
    
    # Load dataset
    dataset = BrowseCompDataset(num_examples=args.num_examples)
    examples = dataset.load()
    print(f"üìö Loaded {len(examples)} examples")

    # GEPA optimization if requested
    if args.optimize:
        print(f"\nüß¨ GEPA Optimization ({args.optimize_steps} steps)")
        split_idx = int(len(examples) * args.train_size)
        train, test = examples[:split_idx], examples[split_idx:]
        print(f"üìä Split: {len(train)} train, {len(test)} test")
        
        program = BrowseCompProgram(agent_factory)
        optimizer = GEPA(metric=metric_fn, steps=args.optimize_steps)
        optimized = optimizer.compile(student=program, trainset=train)
        
        print(f"\n‚ú® Optimization complete!")
        print(f"üìù Optimized {len(list(optimized.named_predictors()))} predictor(s)")
        for name, pred in optimized.named_predictors():
            instr = getattr(pred.signature, 'instructions', '<no instructions>')
            print(f"  ‚Ä¢ {name}: {instr[:80]}..." if len(instr) > 80 else f"  ‚Ä¢ {name}: {instr}")
        
        program = optimized
        examples = test  # Evaluate on test set
    else:
        program = BrowseCompProgram(agent_factory)

    # Run evaluation
    print(f"üöÄ Evaluating...")
    evaluator = dspy.Evaluate(
        devset=examples,
        metric=metric_fn,
        num_threads=args.num_threads,
        display_progress=True,
        display_table=5,
        max_errors=10,
    )
    
    result = evaluator(program)

    # Collect predictions for saving (re-run to capture full prediction objects)
    print("\nüì¶ Collecting detailed results...")
    predictions = []
    for example in examples:
        try:
            pred = program(example.problem)
            predictions.append(pred)
        except Exception as e:
            print(f"‚ö†Ô∏è Error collecting prediction: {e}")
            # Create empty prediction with error marker
            pred = dspy.Prediction(answer="ERROR", report="ERROR")
            pred.metrics = {"accuracy": 0.0, "elapsed_seconds": 0, "total_cost_usd": 0}
            predictions.append(pred)
    
    # Save structured results
    save_experiment_results(
        result=result,
        examples=examples,
        predictions=predictions,
        config=config,
        args=args,
        output_dir="experiments"
    )

    print("\n" + "=" * 50)
    print(f"üìà {args.metric.title()} Score: {result.score:.4f}")
    print(f"üìä Examples: {len(examples)}")
    if args.optimize:
        print(f"üß¨ Optimized with GEPA ({args.optimize_steps} steps)")


if __name__ == "__main__":
    main()
