{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Lead Agent Evaluation Testing\n",
    "\n",
    "This notebook contains unit tests and integration tests for the evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('.')\n",
    "\n",
    "from eval import (\n",
    "    DelegationQualityJudge, ResourceEfficiencyJudge, \n",
    "    QueryClassificationJudge, PlanCoherenceJudge,\n",
    "    LeadAgentEvaluator, EvaluationExample, EvaluationResult,\n",
    "    create_sample_evaluation_data, composite_metric\n",
    ")\n",
    "from agent import QueryAnalysis, ResearchPlan, PlanStep\n",
    "import dspy\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: DSPy Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DSPy configuration\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OPENROUTER_BASE_URL = os.getenv(\"OPENROUTER_BASE_URL\", \"https://openrouter.ai/api/v1\")\n",
    "EVAL_MODEL = os.getenv(\"EVAL_MODEL\", \"anthropic/claude-3.5-sonnet\")\n",
    "\n",
    "assert OPENROUTER_API_KEY, \"OPENROUTER_API_KEY not found in environment\"\n",
    "print(f\"âœ… API Key found: {OPENROUTER_API_KEY[:10]}...\")\n",
    "print(f\"âœ… Base URL: {OPENROUTER_BASE_URL}\")\n",
    "print(f\"âœ… Model: {EVAL_MODEL}\")\n",
    "\n",
    "# Configure DSPy\n",
    "eval_lm = dspy.LM(\n",
    "    model=EVAL_MODEL,\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    api_base=OPENROUTER_BASE_URL\n",
    ")\n",
    "dspy.configure(lm=eval_lm)\n",
    "print(\"âœ… DSPy configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Sample Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sample data creation\n",
    "sample_data = create_sample_evaluation_data()\n",
    "\n",
    "assert len(sample_data) > 0, \"No sample data created\"\n",
    "print(f\"âœ… Created {len(sample_data)} sample examples\")\n",
    "\n",
    "# Verify structure of first example\n",
    "first_example = sample_data[0]\n",
    "print(f\"\\nFirst example:\")\n",
    "print(f\"  Query: {first_example.query}\")\n",
    "print(f\"  Type: {first_example.analysis.query_type}\")\n",
    "print(f\"  Complexity: {first_example.analysis.complexity}\")\n",
    "print(f\"  Steps: {len(first_example.plan.steps)}\")\n",
    "print(\"âœ… Sample data structure is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Individual Judge Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual judge signatures\n",
    "example = sample_data[0]  # Simple query: \"What is the population of Tokyo?\"\n",
    "\n",
    "# Convert to JSON strings for judges\n",
    "analysis_str = example.analysis.model_dump_json()\n",
    "plan_str = example.plan.model_dump_json()\n",
    "\n",
    "print(f\"Testing with query: {example.query}\")\n",
    "print(f\"Analysis JSON length: {len(analysis_str)} chars\")\n",
    "print(f\"Plan JSON length: {len(plan_str)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DelegationQualityJudge\n",
    "print(\"\\nðŸ§ª Testing DelegationQualityJudge...\")\n",
    "delegation_judge = dspy.ChainOfThought(DelegationQualityJudge)\n",
    "\n",
    "try:\n",
    "    delegation_result = delegation_judge(\n",
    "        query=example.query,\n",
    "        analysis=analysis_str,\n",
    "        plan=plan_str\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Delegation Score: {delegation_result.delegation_score}\")\n",
    "    print(f\"âœ… Reasoning: {delegation_result.reasoning}\")\n",
    "    assert 0.0 <= delegation_result.delegation_score <= 1.0, \"Score out of range\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ DelegationQualityJudge failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ResourceEfficiencyJudge\n",
    "print(\"\\nðŸ§ª Testing ResourceEfficiencyJudge...\")\n",
    "efficiency_judge = dspy.ChainOfThought(ResourceEfficiencyJudge)\n",
    "\n",
    "try:\n",
    "    efficiency_result = efficiency_judge(\n",
    "        query=example.query,\n",
    "        complexity=example.analysis.complexity,\n",
    "        plan=plan_str\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Efficiency Score: {efficiency_result.efficiency_score}\")\n",
    "    print(f\"âœ… Reasoning: {efficiency_result.reasoning}\")\n",
    "    assert 0.0 <= efficiency_result.efficiency_score <= 1.0, \"Score out of range\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ResourceEfficiencyJudge failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test QueryClassificationJudge\n",
    "print(\"\\nðŸ§ª Testing QueryClassificationJudge...\")\n",
    "classification_judge = dspy.ChainOfThought(QueryClassificationJudge)\n",
    "\n",
    "try:\n",
    "    classification_result = classification_judge(\n",
    "        query=example.query,\n",
    "        predicted_type=example.analysis.query_type,\n",
    "        analysis=analysis_str\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Classification Score: {classification_result.classification_score}\")\n",
    "    print(f\"âœ… Reasoning: {classification_result.reasoning}\")\n",
    "    assert 0.0 <= classification_result.classification_score <= 1.0, \"Score out of range\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ QueryClassificationJudge failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PlanCoherenceJudge\n",
    "print(\"\\nðŸ§ª Testing PlanCoherenceJudge...\")\n",
    "coherence_judge = dspy.ChainOfThought(PlanCoherenceJudge)\n",
    "\n",
    "try:\n",
    "    coherence_result = coherence_judge(\n",
    "        query=example.query,\n",
    "        analysis=analysis_str,\n",
    "        plan=plan_str\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Coherence Score: {coherence_result.coherence_score}\")\n",
    "    print(f\"âœ… Reasoning: {coherence_result.reasoning}\")\n",
    "    assert 0.0 <= coherence_result.coherence_score <= 1.0, \"Score out of range\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ PlanCoherenceJudge failed: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nâœ… All individual judges working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Full Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete evaluation pipeline\n",
    "print(\"\\nðŸ§ª Testing Full Evaluation Pipeline...\")\n",
    "\n",
    "evaluator = LeadAgentEvaluator()\n",
    "\n",
    "try:\n",
    "    result = evaluator.forward(example)\n",
    "    \n",
    "    print(f\"âœ… Overall Score: {result.overall_score:.3f}\")\n",
    "    print(f\"âœ… Delegation: {result.delegation_score:.3f}\")\n",
    "    print(f\"âœ… Efficiency: {result.efficiency_score:.3f}\")\n",
    "    print(f\"âœ… Classification: {result.classification_score:.3f}\")\n",
    "    print(f\"âœ… Coherence: {result.coherence_score:.3f}\")\n",
    "    \n",
    "    # Verify result structure\n",
    "    assert isinstance(result, EvaluationResult), \"Wrong result type\"\n",
    "    assert 0.0 <= result.overall_score <= 1.0, \"Overall score out of range\"\n",
    "    assert len(result.reasoning) == 4, \"Missing reasoning entries\"\n",
    "    \n",
    "    print(\"\\nâœ… Full evaluation pipeline working correctly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Full evaluation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Evaluate All Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation on all sample data\n",
    "print(\"\\nðŸ§ª Testing All Sample Data...\")\n",
    "\n",
    "results = []\n",
    "for i, example in enumerate(sample_data):\n",
    "    print(f\"\\nEvaluating Example {i+1}: {example.query[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        result = evaluator.forward(example)\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Overall: {result.overall_score:.3f}\")\n",
    "        print(f\"  Delegation: {result.delegation_score:.3f}\")\n",
    "        print(f\"  Efficiency: {result.efficiency_score:.3f}\")\n",
    "        print(f\"  Classification: {result.classification_score:.3f}\")\n",
    "        print(f\"  Coherence: {result.coherence_score:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Calculate averages\n",
    "avg_overall = sum(r.overall_score for r in results) / len(results)\n",
    "avg_delegation = sum(r.delegation_score for r in results) / len(results)\n",
    "avg_efficiency = sum(r.efficiency_score for r in results) / len(results)\n",
    "avg_classification = sum(r.classification_score for r in results) / len(results)\n",
    "avg_coherence = sum(r.coherence_score for r in results) / len(results)\n",
    "\n",
    "print(f\"\\nðŸ“Š Average Scores:\")\n",
    "print(f\"  Overall: {avg_overall:.3f}\")\n",
    "print(f\"  Delegation: {avg_delegation:.3f}\")\n",
    "print(f\"  Efficiency: {avg_efficiency:.3f}\")\n",
    "print(f\"  Classification: {avg_classification:.3f}\")\n",
    "print(f\"  Coherence: {avg_coherence:.3f}\")\n",
    "\n",
    "print(\"\\nâœ… All sample data evaluated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Composite Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test composite metric function\n",
    "print(\"\\nðŸ§ª Testing Composite Metric Function...\")\n",
    "\n",
    "# Create a mock prediction object\n",
    "class MockPrediction:\n",
    "    def __init__(self, analysis, plan):\n",
    "        self.analysis = analysis\n",
    "        self.plan = plan\n",
    "\n",
    "example = sample_data[0]\n",
    "mock_prediction = MockPrediction(example.analysis, example.plan)\n",
    "\n",
    "try:\n",
    "    score = composite_metric(example, mock_prediction)\n",
    "    \n",
    "    print(f\"âœ… Composite metric score: {score:.3f}\")\n",
    "    assert 0.0 <= score <= 1.0, \"Composite score out of range\"\n",
    "    assert isinstance(score, float), \"Score is not a float\"\n",
    "    \n",
    "    print(\"âœ… Composite metric working correctly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Composite metric failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: MIPROv2 Optimizer Setup (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MIPROv2 optimizer setup (without running full optimization)\n",
    "print(\"\\nðŸ§ª Testing MIPROv2 Optimizer Setup...\")\n",
    "\n",
    "try:\n",
    "    from dspy.teleprompt import MIPROv2\n",
    "    from eval import create_optimizer\n",
    "    \n",
    "    # Create optimizer with minimal settings\n",
    "    optimizer = create_optimizer(sample_data)\n",
    "    \n",
    "    print(f\"âœ… MIPROv2 optimizer created successfully\")\n",
    "    print(f\"  Type: {type(optimizer)}\")\n",
    "    print(f\"  Auto mode: light\")\n",
    "    print(f\"  Trials: 20\")\n",
    "    \n",
    "    # Verify it's the right type\n",
    "    assert isinstance(optimizer, MIPROv2), \"Wrong optimizer type\"\n",
    "    \n",
    "    print(\"âœ… MIPROv2 setup working correctly!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸  MIPROv2 import failed - might not be available in this DSPy version: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ MIPROv2 setup failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸŽ‰ All Tests Complete!\")\n",
    "print(\"\\nâœ… Passed Tests:\")\n",
    "print(\"  - DSPy configuration\")\n",
    "print(\"  - Sample data creation\")\n",
    "print(\"  - Individual judge signatures\")\n",
    "print(\"  - Full evaluation pipeline\")\n",
    "print(\"  - All sample data evaluation\")\n",
    "print(\"  - Composite metric function\")\n",
    "print(\"  - MIPROv2 optimizer setup\")\n",
    "\n",
    "print(\"\\nðŸš€ Ready for production use!\")\n",
    "print(\"\\nðŸ“ Next steps:\")\n",
    "print(\"  - Run actual MIPROv2 optimization with larger dataset\")\n",
    "print(\"  - Integrate with agent.py for end-to-end optimization\")\n",
    "print(\"  - Add more diverse evaluation examples\")\n",
    "print(\"  - Tune optimization hyperparameters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}